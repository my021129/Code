import os
import numpy as np
import matplotlib.pyplot as plt
from itertools import chain

import torch
from torch.utils.data import DataLoader
from torchvision.transforms import Compose, ToTensor, Normalize, Resize

from model import Siren, Homography  # ä½ è‡ªå®šä¹‰çš„æ¨¡å‹æ¨¡å—
from util import get_mgrid, apply_homography, VideoFitting  # è‡ªå®šä¹‰å·¥å…·å‡½æ•°


# ========================
# ğŸ§  å•åº”æ€§ + SIREN ç½‘ç»œè®­ç»ƒå‡½æ•°
# ========================
def train_homography(path, total_steps=3000, verbose=True, steps_til_summary=100):
    transform = Compose([
        Resize(512),
        ToTensor(),
        Normalize(torch.tensor([0.5] * 3), torch.tensor([0.5] * 3))
    ])

    dataset = VideoFitting(path, transform)
    dataloader = DataLoader(dataset, batch_size=1, num_workers=0, pin_memory=True)

    g = Homography(hidden_features=256, hidden_layers=2).cuda()
    f = Siren(in_features=2, out_features=3, hidden_features=256,
              hidden_layers=4, outermost_linear=True).cuda()

    optim = torch.optim.Adam(chain(g.parameters(), f.parameters()), lr=1e-4)

    model_input, ground_truth = next(iter(dataloader))
    model_input, ground_truth = model_input[0].cuda(), ground_truth[0].cuda()

    batch_size = (dataset.H * dataset.W) // 4
    loss_history = []  # æ–°å¢ï¼šè®°å½•æ¯æ­¥æŸå¤±

    for step in range(total_steps):
        start = (step * batch_size) % len(model_input)
        end = min(start + batch_size, len(model_input))

        xy = model_input[start:end, :-1]
        t = model_input[start:end, -1:]

        h = g(t)
        xy_warped = apply_homography(xy, h)
        pred = f(xy_warped)

        loss = ((pred - ground_truth[start:end]) ** 2).mean()
        loss_history.append(loss.item())  # ä¿å­˜æŸå¤±

        if verbose and step % steps_til_summary == 0:
            print(f"Step [{step:04d}/{total_steps}]: loss={loss.item():.4f}")

        optim.zero_grad()
        loss.backward()
        optim.step()

    return f, loss_history  # è¿”å›æŸå¤±è®°å½•



# ğŸš€ è®­ç»ƒæ¨¡å‹
f, loss_history = train_homography('./data/vis/homography', total_steps=3000)

# ğŸ“‰ ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
plt.figure(figsize=(10, 4))
plt.plot(loss_history, label='Training Loss', color='blue')
plt.xlabel('Training Step')
plt.ylabel('MSE Loss')
plt.title('Training Loss Curve')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()


# ğŸ¨ å¯è§†åŒ–é‡å»ºå›¾åƒ
with torch.no_grad():
    xy = get_mgrid([512, 1024], [-1.5, -2.0], [1.5, 2.0]).cuda()
    output = f(xy).view(512, 1024, 3).cpu().numpy()
    output = np.clip(output, -1, 1) * 0.5 + 0.5

    plt.figure(figsize=(18, 18))
    plt.imshow(output)
    plt.axis('off')
    plt.show()

